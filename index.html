<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-PrefDrive Project</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            text-align: center;
            margin: 0;
            padding: 20px;
        }

        h1 {
            font-size: 28px;
            color: #333;
            margin-bottom: 10px;
        }

        h2 {
            font-size: 22px;
            color: #555;
        }

        p {
            font-size: 16px;
            color: #666;
            line-height: 1.5;
        }

        .navigation {
            margin: 20px 0;
        }

        .navigation a {
            margin: 0 15px;
            text-decoration: none;
            color: #007BFF;
            font-weight: bold;
        }

        .navigation a:hover {
            text-decoration: underline;
        }

        .visuals {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 20px;
            margin: 30px 0;
        }

        .visuals figure {
            margin: 0;
            padding: 0;
            width: 80%;
            max-width: 800px;
            display: inline-block;
            vertical-align: top;
        }

        .visuals figure img,
        .visuals figure video {
            width: 100%;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin-bottom: 8px;
        }

        .visuals figcaption {
            font-size: 14px;
            color: #555;
            margin-top: 5px;
            font-style: italic;
            text-align: center;
        }

        .overview {
            max-width: 900px;
            margin: 20px auto;
            text-align: left;
            padding: 0 20px;
        }

        .table-container {
            margin: 30px auto;
            padding: 15px;
            background-color: #f9f9f9;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            max-width: 900px;
            overflow-x: auto;
        }

        .table-caption {
            font-weight: bold;
            margin-bottom: 15px;
            text-align: center;
            font-size: 18px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 0 auto;
        }

        th, td {
            padding: 8px 12px;
            border: 1px solid #ddd;
            text-align: center;
        }

        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }

        .highlightedRow {
            background-color: #f5f5f5;
        }

        .best {
            font-weight: bold;
        }

        .second-best {
            text-decoration: underline;
        }

        .footnote {
            font-size: 14px;
            font-style: italic;
            text-align: center;
            margin-top: 10px;
        }

        @media (max-width: 768px) {
            .visuals figure {
                width: 95%;
            }
        }
    </style>
</head>
<body>
    <h1>Multi-PrefDrive: Optimizing Language Models for Autonomous Driving Through Multi-Preference Tuning</h1>
    <p><strong>Authors:</strong> Yun Li, Ehsan Javanmardi, Simon Thompson, Kai Katsumata, Alex Orsholits, Manabu Tsukada</p>
    <p><strong>Affiliations:</strong> Graduate School of Information and Science Technology, The University of Tokyo, Tokyo, Japan; TIER IV, Inc., Tokyo, Japan</p>

    <div class="navigation">
        <a href="path/to/paper.pdf" target="_blank">Paper</a>
        <a href="https://github.com/your-repo" target="_blank">Code</a>
        <a href="https://arxiv.org/abs/your-paper" target="_blank">arXiv</a>
        <a href="https://huggingface.co/datasets/liyun0607/Multi-PrefDrive" target="_blank">Dataset</a>
    </div>

    <div class="visuals">
        <figure>
            <video controls>
                <source src="videos/DPO-success.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Demonstration of preference tuning-based methods</figcaption>
        </figure>
        <figure>
            <video controls>
                <source src="videos/LMDrive-fail.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Demonstration of LMDrive (baseline)</figcaption>
        </figure>
    </div>

    <div class="overview">
        <h2>Abstract</h2>
        <p>This paper introduces Multi-PrefDrive, a framework that significantly enhances LLM-based autonomous driving through multidimensional preference tuning. Aligning LLMs with human driving preferences is crucial yet challenging, as driving scenarios involve complex decisions where multiple incorrect actions can correspond to a single correct choice. Traditional binary preference tuning fails to capture this complexity. Our approach pairs each chosen action with multiple rejected alternatives, better reflecting real-world driving decisions. By implementing the Plackett-Luce preference model, we enable nuanced ranking of actions across the spectrum of possible errors. Experiments in the CARLA simulator demonstrate that our algorithm achieves an 11.0% improvement in overall score and an 83.6% reduction in infrastructure collisions, while showing perfect compliance with traffic signals in certain environments. Comparative analysis against DPO and its variants reveals that Multi-PrefDrive's superior discrimination between chosen and rejected actions, which achieving a margin value of 25, and such ability has been directly translates to enhanced driving performance. We implement memory-efficient techniques including LoRA and 4-bit quantization to enable deployment on consumer-grade hardware and will open-source our training code and multi-rejected dataset to advance research in LLM-based autonomous driving systems.</p>

        <h2>Multi-PrefDrive Framework</h2>
        <div class="visuals">
            <figure>
                <img src="figures/system-structure.jpg" alt="Architecture overview of the Multi-PrefDrive framework">
                <figcaption>Fig. 1: Architectural overview of the Multi-PrefDrive framework, which consists of four principal components: a multimodal perception module, a language representation module, a preference tuning component, and an action execution module.</figcaption>
            </figure>
        </div>

        <h2>Experiments</h2>
        <div class="visuals">
            <figure>
                <img src="figures/training_comparison.jpg" alt="Training comparison of different methods">
                <figcaption>Fig. 2: Comparison of training metrics across different preference learning methods, showing PLDPO's superior performance in discriminating between chosen and rejected actions.</figcaption>
            </figure>
        </div>

        <div class="table-container">
            <div class="table-caption">Table 1: Comprehensive Analysis of LangAuto Benchmarks in CARLA Town 01 and Town 04</div>
            <table>
                <thead>
                    <tr>
                        <th rowspan="2">Method</th>
                        <th>Overall</th>
                        <th>Safety</th>
                        <th>Navigation</th>
                        <th>Infrastructure</th>
                        <th>Red Light</th>
                        <th>Route</th>
                        <th>Vehicle</th>
                    </tr>
                    <tr>
                        <th>Score (↑)</th>
                        <th>Metric (↑)</th>
                        <th>Score (↑)</th>
                        <th>Collisions (↓)</th>
                        <th>Infractions (↓)</th>
                        <th>Deviation (↓)</th>
                        <th>Blocked (↓)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlightedRow">
                        <td colspan="8"><strong>Town 01</strong></td>
                    </tr>
                    <tr>
                        <td>LMDrive (baseline)</td>
                        <td>53.00</td>
                        <td>0.86</td>
                        <td>59.10</td>
                        <td>0.73</td>
                        <td>0.22</td>
                        <td class="second-best">1.32</td>
                        <td>0.11</td>
                    </tr>
                    <tr>
                        <td>DPO</td>
                        <td class="second-best">56.12 (↑5.9%)</td>
                        <td>0.88 (↑2.3%)</td>
                        <td class="best">64.15 (↑8.5%)</td>
                        <td>0.27 (↓63.5%)</td>
                        <td class="second-best">0.16 (↓28.1%)</td>
                        <td>1.36 (↑3.0%)</td>
                        <td class="best">0.00 (↓100.0%)</td>
                    </tr>
                    <tr>
                        <td>NoPrefDPO</td>
                        <td>51.45 (↓2.9%)</td>
                        <td>0.91 (↑5.8%)</td>
                        <td>55.17 (↓6.7%)</td>
                        <td>0.61 (↓17.0%)</td>
                        <td>0.20 (↓11.9%)</td>
                        <td>1.74 (↑31.7%)</td>
                        <td>0.08 (↓25.0%)</td>
                    </tr>
                    <tr>
                        <td>BCO</td>
                        <td>54.48 (↑2.8%)</td>
                        <td>0.91 (↑5.8%)</td>
                        <td>59.42 (↑0.5%)</td>
                        <td class="second-best">0.25 (↓65.8%)</td>
                        <td>0.20 (↓8.7%)</td>
                        <td>1.60 (↑21.4%)</td>
                        <td class="best">0.00 (↓100.0%)</td>
                    </tr>
                    <tr>
                        <td>IPO</td>
                        <td>52.93 (↓0.1%)</td>
                        <td>0.91 (↑5.8%)</td>
                        <td>56.87 (↓3.8%)</td>
                        <td>0.47 (↓35.0%)</td>
                        <td class="best">0.15 (↓31.6%)</td>
                        <td>1.78 (↑34.9%)</td>
                        <td>0.11 (0.0%)</td>
                    </tr>
                    <tr>
                        <td>PLDPO</td>
                        <td class="best">58.85 (↑11.0%)</td>
                        <td class="best">0.92 (↑7.0%)</td>
                        <td class="second-best">63.59 (↑7.6%)</td>
                        <td class="best">0.12 (↓83.6%)</td>
                        <td class="best">0.15 (↓32.3%)</td>
                        <td class="best">1.15 (↓12.9%)</td>
                        <td class="best">0.00 (↓100.0%)</td>
                    </tr>
                    <tr class="highlightedRow">
                        <td colspan="8"><strong>Town 04</strong></td>
                    </tr>
                    <tr>
                        <td>LMDrive (baseline)</td>
                        <td>60.11</td>
                        <td>0.93</td>
                        <td>65.25</td>
                        <td>0.00</td>
                        <td>0.24</td>
                        <td>1.86</td>
                        <td>0.00</td>
                    </tr>
                    <tr>
                        <td>DPO</td>
                        <td class="second-best">62.81 (↑4.5%)</td>
                        <td class="best">0.95 (↑2.2%)</td>
                        <td class="second-best">67.59 (↑3.6%)</td>
                        <td>0.00 (0.0%)</td>
                        <td>0.12 (↓49.3%)</td>
                        <td>1.84 (↓0.7%)</td>
                        <td>0.00 (0.0%)</td>
                    </tr>
                    <tr>
                        <td>NoPrefDPO</td>
                        <td>62.27 (↑3.6%)</td>
                        <td>0.94 (↑1.1%)</td>
                        <td class="best">68.35 (↑4.7%)</td>
                        <td>0.00 (0.0%)</td>
                        <td class="second-best">0.05 (↓80.1%)</td>
                        <td>1.77 (↓4.6%)</td>
                        <td>0.00 (0.0%)</td>
                    </tr>
                    <tr>
                        <td>BCO</td>
                        <td>57.63 (↓4.1%)</td>
                        <td>0.94 (↑1.1%)</td>
                        <td>62.17 (↓4.7%)</td>
                        <td>0.00 (0.0%)</td>
                        <td>0.19 (↓17.9%)</td>
                        <td>1.86 (0.0%)</td>
                        <td>0.00 (0.0%)</td>
                    </tr>
                    <tr>
                        <td>IPO</td>
                        <td>62.12 (↑3.3%)</td>
                        <td>0.94 (↑1.1%)</td>
                        <td>67.34 (↑3.2%)</td>
                        <td>0.00 (0.0%)</td>
                        <td>0.08 (↓65.8%)</td>
                        <td class="second-best">1.76 (↓5.0%)</td>
                        <td>0.00 (0.0%)</td>
                    </tr>
                    <tr>
                        <td>PLDPO</td>
                        <td class="best">63.69 (↑6.0%)</td>
                        <td class="best">0.95 (↑2.2%)</td>
                        <td class="best">68.35 (↑4.7%)</td>
                        <td>0.00 (0.0%)</td>
                        <td class="best">0.00 (↓100.0%)</td>
                        <td class="best">1.75 (↓5.5%)</td>
                        <td>0.00 (0.0%)</td>
                    </tr>
                </tbody>
            </table>
            <div class="footnote">Note: Bold values indicate the best performance for each metric in each town. Underlined values indicate the second-best overall score.</div>
        </div>

        <div class="table-container">
            <div class="table-caption">Table 2: Training Configurations</div>
            <table>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Base Model</td>
                        <td>LLaMA-7B</td>
                    </tr>
                    <tr>
                        <td>Training Strategy</td>
                        <td>LoRA</td>
                    </tr>
                    <tr>
                        <td>LoRA Rank (r)</td>
                        <td>16</td>
                    </tr>
                    <tr>
                        <td>LoRA Alpha (α)</td>
                        <td>16</td>
                    </tr>
                    <tr>
                        <td>Learning Rate</td>
                        <td>1e-5</td>
                    </tr>
                    <tr>
                        <td>Batch Size</td>
                        <td>Auto-calculated based on GPU memory</td>
                    </tr>
                    <tr>
                        <td>Gradient Accumulation Steps</td>
                        <td>max(1, 32 // batch_size)</td>
                    </tr>
                    <tr>
                        <td>Training Epochs</td>
                        <td>3</td>
                    </tr>
                    <tr>
                        <td>Maximum Sequence Length</td>
                        <td>2,048</td>
                    </tr>
                    <tr>
                        <td>Warmup Ratio</td>
                        <td>0.1</td>
                    </tr>
                    <tr>
                        <td>Max Gradient Norm</td>
                        <td>0.3</td>
                    </tr>
                    <tr>
                        <td>DPO β</td>
                        <td>Scheduled (0.05-0.2)</td>
                    </tr>
                    <tr class="highlightedRow">
                        <td colspan="2"><strong>LoRA Target Modules</strong></td>
                    </tr>
                    <tr>
                        <td colspan="2">q_proj, k_proj, v_proj, o_proj, gate_proj, down_proj, up_proj</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>Conclusion</h2>
        <p>Our work with Multi-PrefDrive advances autonomous driving systems by addressing a fundamental limitation in preference learning approaches. By recognizing that real-world driving scenarios present numerous potential errors of varying severity rather than simple binary choices, we've developed a framework that captures this complexity through multi-rejected preference tuning. The experimental results demonstrate that our PLDPO approach outperforms conventional DPO variants with significant improvements in safety-critical metrics, and achieving perfect compliance with traffic rules in certain scenarios. The clear correlation between the larger preference margins learned during training and improved driving performance validates our approach's theoretical foundations. Moreover, through memory-efficient implementation techniques, we've made this sophisticated preference learning accessible to researchers without expensive hardware, and our forthcoming open-source code and multi-rejected dataset will enable broader exploration of this promising direction. As autonomous driving systems continue to advance, we believe multi-dimensional preference learning will be crucial for developing vehicles that can navigate complex urban environments with human-like judgment and safety prioritization.</p>
    </div>
</body>
</html>
